---
title: "Bayesian Analysis On Travel Insurance"
author: "HungChun Lin"
date: "12/8/2019"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r,warning=FALSE,message=FALSE}
source("CodeSource.R")
#load packages
loadPkg("sjmisc")
loadPkg("ggplot2")
loadPkg("plotly")
loadPkg("dplyr")
loadPkg("dummies")
loadPkg("kableExtra")
loadPkg("bestglm")
loadPkg("glmnet")
loadPkg("stats")
loadPkg("caTools")
loadPkg("RColorBrewer")
loadPkg("ROSE")
```

## Problem statement
### Background
As the living quality improved, nowadays many people consider traveling as their first choice
to spend their spare time. That should be an unforgettable experience traveling with family
and friends, but we often hear bad news about property loss, injury or even death during
traveling. Although being careful is important to avoid danger from traveling, choosing a
proper destination, safe transportation, reliable agency could also be a good way to
protect ourselves.

So what factors might cause the accidents during traveling?

### Proposed Solution
Apply Bayesian analysis on travel insurance dataset.  

## Dataset
[Travel insurance](https://www.kaggle.com/mhdzahier/travel-insurance) dataset from Kaggle, Provided by a third-party insurance servicing company based in Singapore.

```{r}
#Load the data
insurance_data = loadData('travel_insurance.csv')

#Summarize dataset
summary(insurance_data)

#Rename the column
names(insurance_data)[names(insurance_data) == "Commision..in.value."] <- "Commision"
```
Totally 63326 records, 11 variables.  
Target: Claim status (YES/NO), Claim status of insurance could indicate whether the customer encountered accident during traveling.  
Features: Agency, Agency type, Distribution channel, Product name, Duration, Destination, Net sales, Commission, Gender, Age.  


## Data preprocessing
### Check for null values
To many empty value in *Gender*, drop column directly
```{r}
##Drop NA
Gender_empty_list = checkEmpty(insurance_data$Gender)
print(length(Gender_empty_list))
###To many empty value in Gender, drop it directly
drop_columns = c("Gender") 
insurance_data = insurance_data[, !(names(insurance_data) %in% drop_columns)]
```
### Drop outlier for continuous variables
#### Age
```{r}
####make boxplot
age_data = insurance_data["Age"]
age_plot = box_plot(age_data, 'red')
####drop outlier
insurance_data = insurance_data[!(insurance_data$Age < 0 & insurance_data$Age >100),]
```
#### Duration
```{r}
####make boxplot
duration_data = insurance_data["Duration"]
duration_plot = box_plot(duration_data, 'red')
####drop outlier
insurance_data = insurance_data[!(insurance_data$Duration <= 0 & insurance_data$Duration > 4000),]
```

#### Commission
```{r}
commision_data = insurance_data["Commision"]
####make boxplot
commission_plot = box_plot(commision_data, 'green')
```

#### Net.Sales
```{r}
sales_data = insurance_data["Net.Sales"]
####make boxplot
sales_plot = box_plot(sales_data, 'yellow')
####drop outliers
insurance_data<-insurance_data[!(insurance_data$Net.Sales <= 0),]
```
```{r}
summary(insurance_data)
```
### Label encoding 
For the taget "Claim", we do label encoding, "Yes" to 1, "No" to 0.
```{r}
##Label encoder the target
insurance_data$Claim = as.factor(insurance_data$Claim) 
levels(insurance_data$Claim) <- c(0,1)
```    
## EDA
For the target, "Claim"
```{r}
no_count = 0
yes_count = 0
for(element in insurance_data$Claim){
  if (element==0){
    no_count = no_count+1
  }
  else{
    yes_count = yes_count + 1
  }

}
print("Yes:")
print(yes_count/length(insurance_data$Claim))
print("No:")
print(no_count/length(insurance_data$Claim))
```
Two levels, 62399 (98.5%) “No”, 927 (1.5%) “Yes”. They are imbalanced, so we do upsampling but the performance become worse.  

For the caregorical features,
```{r}
#Descriptive Statistics analysis
n_Agency = length(unique(insurance_data$Agency)) ##15
n_Agency
n_Agency_type = length(unique(insurance_data$Agency.Type)) ##2
n_Agency_type
n_Destination = length(unique(insurance_data$Destination)) ## 147
n_Destination
n_Distribution_type = length(unique(insurance_data$Distribution.Channel)) ##2
n_Distribution_type
n_Product_name = length(unique(insurance_data$Product.Name)) ##26
n_Product_name
```
“Agency”: 15 levels, “Destination”: 147 levels, “Product name”: 26 levels.

```{r}
Agency_freq = insurance_data %>% 
  count(Agency, Claim, sort = TRUE) %>% 
  top_n(15)
Agency_type_freq = insurance_data %>% 
  count(Agency.Type, Claim, sort = TRUE) %>% 
  top_n(10)
Destination_type_freq = insurance_data %>% 
  count(Destination, Claim, sort = TRUE) %>% 
  top_n(21) 
Product_name_freq = insurance_data %>% 
  count(Product.Name, Claim, sort = TRUE) %>% 
  top_n(22)
Distribution_type_freq = insurance_data %>% 
  count(Distribution.Channel, Claim, sort = TRUE) %>% 
  top_n(4)

ggplot(data = Destination_type_freq, aes(x = reorder(Destination, -n), y = n)) + 
  geom_bar(stat="identity", aes(fill = Claim))  +
  ggtitle("Claim or not among the Top 20 Destination") + 
  theme(axis.text.x = element_text(angle = 0, hjust = 1), 
        axis.title = element_blank()) +
  coord_flip()

ggplot(data = Agency_type_freq, aes(x = reorder(Agency.Type, -n), y = n)) + 
  geom_bar(stat="identity", aes(fill = Claim))  +
  ggtitle("Claim or not given the Agency type") + 
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        axis.title = element_blank())

ggplot(data = Distribution_type_freq, aes(x = reorder(Distribution.Channel, -n), y = n)) + 
  geom_bar(stat="identity", aes(fill = Claim))  +
  ggtitle("Claim or not given the Distribution type") + 
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        axis.title = element_blank())

ggplot(data = Product_name_freq, aes(x = reorder(Product.Name, -n), y = n)) + 
  geom_bar(stat="identity", aes(fill = Claim))  +
  ggtitle("Claim or not among the Product") + 
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        axis.title = element_blank())+
  coord_flip()

ggplot(data = Agency_freq, aes(x = reorder(Agency, -n), y = n)) + 
  geom_bar(stat="identity", aes(fill = Claim))  +
  ggtitle("Claim or not among the Agency") + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        axis.title = element_blank())
```
Too many levels in "Agencys", "Destination" and "Product name", then it will generate too many features after one-hot encoding.  
“Destination”: 147 levels → Top 10 levels + 1 “Others” level  
”Agency”: 15 levels → Keep  
“Product”: Due to the correlation with “Agency” → Drop column  

```{r}
##Too many types of Destination, almost 147 types, so We only keep 10 contries and 1 new dummy called 'others'.
##All the top 10 destinations are larger than 2000
df_Destination_freq = insurance_data %>% 
  group_by(Destination) %>%
  summarise(no_rows = length(Destination))

df_Destination_freq = df_Destination_freq  %>% 
  arrange(desc(no_rows)) ## descending order
top_10_Destination = df_Destination_freq[1:10,1] ##select the top 10
top_10_Destination = as.character(unlist(top_10_Destination))
length(top_10_Destination)
all_Destination = as.character(unique(unlist(insurance_data["Destination"])))
length(all_Destination)
none_top_Destination = all_Destination[!all_Destination %in% top_10_Destination] ##filter the top destination
length(none_top_Destination)

insurance_data$Destination = as.character(insurance_data$Destination)
insurance_data$Destination = ifelse(insurance_data$Destination %in% none_top_Destination, "Others", insurance_data$Destination)
insurance_data$Destination = as.factor(insurance_data$Destination)

#Product name
#There are 26 types in product name, some have fewer records, so we only keep top 10 product, and add 1 new dummy call "Others"
df_Product_freq = insurance_data %>% 
  group_by(Product.Name) %>%
  summarise(no_rows = length(Product.Name))

df_Product_freq = df_Product_freq  %>% 
  arrange(desc(no_rows)) ## descending order

top_10_Product = df_Product_freq[1:10,1] ##select the top 10
top_10_Product = as.character(unlist(top_10_Product))
length(top_10_Product)
all_Prouct = as.character(unique(unlist(insurance_data["Product.Name"])))
length(all_Prouct)
none_top_Prouct = all_Prouct[!all_Prouct %in% top_10_Product] ##filter the top destination
length(none_top_Prouct)

insurance_data$Product.Name = as.character(insurance_data$Product.Name)
insurance_data$Product.Name = ifelse(insurance_data$Product.Name %in% none_top_Prouct, "Others", insurance_data$Product.Name)
insurance_data$Product.Name = as.factor(insurance_data$Product.Name)

##Drop product.Name column due to multicorrelation with Agency
drop_column_agency = c("Product.Name") 
insurance_data = insurance_data[, !(names(insurance_data) %in% drop_column_agency)]
```

```{r}
##Drop product.Name column due to multicorrelation with Agency
drop_column_agency = c("Product.Name") 
insurance_data = insurance_data[, !(names(insurance_data) %in% drop_column_agency)]
```


## Feature Selection
As there are too many categorical variable in our dataset, consider the computing power, we will use lasso to do the feature selection and select the top 15 important features.
```{r}
##Lasso feature selection
###Set x and y in lasso regression
x=model.matrix(Claim~.,insurance_data)[,-1]
y=insurance_data$Claim

###Make the lasso coefficient plot
set.seed(100)
lasso_mod=glmnet(x,y,alpha=1,family="binomial")
lasso_plot1 = plot(lasso_mod, xvar = "lambda", pch=19)

###Check the cross validation 
cv_lasso=cv.glmnet(x,y,alpha=1,family="binomial")
###Check the best lambda and 1 standard error lambda
bestlam=cv_lasso$lambda.1se
bestlam1=cv_lasso$lambda.min
lasso_plot2 = plot(cv_lasso)
lasso_plot2

###Check the coefficient of the variables at 1 standard error point
lasso_coef = coef(cv_lasso, s = "lambda.min")
###Select the top variables
order_data = abs(as.data.frame(as.matrix(lasso_coef)))
order_data$column = row.names(order_data)
row.names(order_data) <- NULL
order_data <- order_data[-1,]
selected_variable = order_data[order(-order_data[1]),][1:15,]
###List the column name of the selected variables
variable_name = selected_variable["column"][1:15,1]
variable_name
```

## Run the Robust Logistic regression
We use JAGS to run the robust logistic regression with target "Claim". Firstly, we need to do one-hot encoding on the categorical features, and select out the top 15 columns selected by Lasso. Then we split the data into training(90%) and testing(10%) dataset. We also check the correlation between features, we can see only AgencyC2B and DestinationSingapore has a little bit higher correlation, but due to actual situation, AgencyC2B client's destination are all to Singapore, but Client travel to Singapore are not all from AgencyC2B, we decide to keep this two features.
```{r}
#For the nominal variable, we do one-hot encoding
dummy_data = dummy.data.frame(insurance_data, name=c("Destination","Agency","Distribution.Channel"), sep="")
dummy_data = as.data.frame(apply(dummy_data, 2, as.numeric))
bys_data = dummy_data[variable_name]
bys_data$Claim = as.numeric(insurance_data$Claim)-1
str(bys_data)

#use caTools function to split, SplitRatio for 90%:10% splitting
dt= sample.split(bys_data,SplitRatio = 0.9)
#subsetting into Train data
train =subset(bys_data,dt==TRUE)
#subsetting into Test data
test =subset(bys_data,dt==FALSE)

#Plot the correlation matrix between variables
cor_matrix = round(cor(x=train, method = c("pearson")),2)
kable(cor_matrix)
mypalette<-brewer.pal(12,"YlGnBu")
heatmap(cor_matrix, Colv = NA, Rowv = NA, scale="column", 
        col= mypalette)
```

Finally, we generate the Robust Logistic regression using 4 chains with 3750 iterations.
```{r eval = FALSE}
#Get the column names for all features
xName = colnames(subset(train, select = -c(Claim)))
yName = "Claim"
#Convert factor to numeric before shipping to JAGS
train = train[,c(xName, yName)]


#Generate the robust MCMC Logistic
mcmcCoda = genMCMC( data = train, xName = xName , yName = yName, 
                    numSavedSteps=15000 , thinSteps=1 , saveName=NULL ,
                    runjagsMethod=runjagsMethodDefault , 
                    nChains=nChainsDefault)

#Display diagnostics of chain, for specified parameters:
parameterNames = varnames(mcmcCoda) # get all parameter names
for ( parName in parameterNames ) {
  diagMCMC( codaObject=mcmcCoda , parName=parName , 
            saveName="auto" , saveType="png" )
}

#Get summary statistics of chain:
summaryInfo = smryMCMC( mcmcCoda , 
                        saveName="Summary_Info" )
show(summaryInfo)

#Display posterior information:
plotMCMC( mcmcCoda , data=train, xName=xName , yName=yName , 
          pairsPlot=TRUE , showCurve=FALSE ,
          saveName="weight"  , saveType="png" )
```
From the distribution of posterior, we can see, there are 7 features beta are not significant, in other words, the HDI of the distribution contains "0", so these features may have beta equals zero, which means these features may not significant when interpreting the target "Claim". 
From each beta's diagnosis, we can see, beta0 and beta5 are converge very good and have good Effective sample size, which mean these two betas are very stable. 
When looking at beta1 and beta11, we can see these two betas are converge very bad, the trace plots are sticky, the autocorrelation are very high, and the ESS are very low. This should be because of the higher correlation of the two features.
## Prediction
Finally, we want to check the accuracy of the MCMC result. We use the 8 significant variables in MCMC to train the model and compare the result with guessing parameter and without guessing parameter. When we train the model in order to imporve the accuracy, we also do the oversampling to balance our target. 
```{r}
knitr::include_graphics("function1.png")
```

```{r}
#subtset the data 
train_pred = train[c("Claim","AgencyC2B","Distribution.ChannelOnline",
                     "AgencyLWC","AgencyTST","AgencyKML","AgencyCWT",
                     "DestinationSINGAPORE","DestinationMALAYSIA")]
test_pred = test[c("Claim","AgencyC2B","Distribution.ChannelOnline",
                     "AgencyLWC","AgencyTST","AgencyKML","AgencyCWT",
                     "DestinationSINGAPORE","DestinationMALAYSIA")]
#oversampling the train data
train_pred <- ovun.sample(Claim~., data=train_pred,
                                seed=1, method="over")$data
#train and test the model
summary(glm_pred <- glm(Claim ~ ., family="binomial", data=train_pred))
pred <- predict(glm_pred, newdata = test_pred, type="response")

#calculate the prediction of robust result
pred_rbust <- pred*(1- 0.01) + 0.01/2
glm_rbust <- ifelse(pred_rbust > 0.5, "Up", "Down")
#plot the confusion matrix of the robust prediction
confus.matrix <- table(real=test_pred$Claim, predict=glm_rbust)
accuracy <- sum(diag(confus.matrix))/sum(confus.matrix)
accuracy
```

```{r}
knitr::include_graphics("function2.png")
```

```{r}
#calculate the prediction of non-robust result
glm_nrbust <- ifelse(pred > 0.5, "Up", "Down")
#plot the confusion matrix of the non-robust prediction
confus.matrix <- table(real=test_pred$Claim, predict=glm_nrbust)
plot(confustion)
accuracy <- sum(diag(confus.matrix))/sum(confus.matrix)
accuracy
```
From the result we can see, the two results are almost the same, as we know when alpha equals to 0, the model is non-robust, just the logistic regression. when alpha equals to 1, the model is a horizontal line with y intercept 1/2. This means our model is very very close to a non-robust model, this should because the value of our guessing parameter is too small, so it is almost has no influence on our model. 

In conclusion, non-robust Logistic regression is good enough to use on our project. In MCMC only 8 of 15 variables are significant. When there are high correlation variables in a dataset, this will impact the convergence of samples in MCMC.

